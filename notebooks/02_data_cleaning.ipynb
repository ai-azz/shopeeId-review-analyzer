{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf72fd8",
   "metadata": {},
   "source": [
    "# Import Library & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9f9e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Raw Data:\n",
      "0    padahal aku nyamaaan banget belaja disini tapi...\n",
      "1                                         bagus bangat\n",
      "2    semua oke sih, cuma saat saya mau ngasih ulasa...\n",
      "3                                   mudah simpel cepat\n",
      "4    maaf untuk sementara saya kasih bintang 1, kar...\n",
      "5                                                bagus\n",
      "6    aplikasi aneh, transaksi selalu dibatalkan sys...\n",
      "7                                               mantap\n",
      "8    sistem sangat bobrok, berulangkali mendapat or...\n",
      "9    Akun tiba tiba diblokir tidak ada penjelasan, ...\n",
      "Name: review_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re\n",
    "from collections import Counter\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import tqdm\n",
    "from tqdm.auto import tqdm \n",
    "tqdm.pandas()\n",
    "\n",
    "# load raw data\n",
    "df = pd.read_csv('../data/raw/shopee_reviews_raw.csv')\n",
    "\n",
    "# check sample data before cleaning\n",
    "print(\"Sample Raw Data:\")\n",
    "print(df['review_text'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720fb8b",
   "metadata": {},
   "source": [
    "# Pre-Cleaning EDA\n",
    "Data Profiling Analysis:\n",
    "- Vocabulary Size Check: Counts the number of unique words in the raw data.\n",
    "- Top Noise Identification: Looks for frequently occurring words that are not standard. This is a resource for scientifically creating key_norms (slang dictionaries).\n",
    "- Emoji Analysis: Sees which emojis appear most frequently. The ðŸ˜Š or ðŸ˜¡ emojis are important features for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d717282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words (Vocabulary) before cleaning: 20985\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size check\n",
    "raw_all_words = df['review_text'].str.lower().str.split().explode().dropna()\n",
    "raw_vocab_size = raw_all_words.nunique()\n",
    "\n",
    "print(f\"Number of unique words (Vocabulary) before cleaning: {raw_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328caebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 most frequent words (check for potential stopwords/slang):\n",
      "          Word  Frequency\n",
      "0           di       4759\n",
      "1       shopee       3995\n",
      "2       sangat       3441\n",
      "3          dan       3377\n",
      "4         saya       2615\n",
      "5        bagus       2598\n",
      "6      belanja       2280\n",
      "7          nya       1997\n",
      "8       barang       1877\n",
      "9           yg       1852\n",
      "10        yang       1706\n",
      "11         ada       1598\n",
      "12       tidak       1595\n",
      "13        bisa       1573\n",
      "14    aplikasi       1510\n",
      "15      mantap       1289\n",
      "16    membantu       1254\n",
      "17  pengiriman       1253\n",
      "18         ini       1216\n",
      "19       untuk       1059\n"
     ]
    }
   ],
   "source": [
    "# check 20 most frequent words (including slang/noise)\n",
    "top_tokens = raw_all_words.value_counts().head(20).rename_axis('Word').reset_index(name='Frequency')\n",
    "\n",
    "print(\"\\n20 most frequent words (check for potential stopwords/slang):\")\n",
    "print(top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c505d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Most frequent emojis:\n",
      "   Emoji  Frequency\n",
      "0      ðŸ‘       1561\n",
      "1      â­        597\n",
      "2      ðŸ™        336\n",
      "3      ðŸ»        244\n",
      "4      ðŸ¥°        217\n",
      "5      ðŸ˜¡        215\n",
      "6      ðŸ‘Ž        121\n",
      "7      ðŸ˜­        110\n",
      "8      â¤         91\n",
      "9      ðŸ˜         90\n",
      "10     ðŸ‘Œ         79\n",
      "11     ðŸ˜Š         69\n",
      "12     ðŸ˜         59\n",
      "13     ðŸ’¯         54\n",
      "14     ðŸ˜˜         42\n",
      "15     ðŸ«°         41\n",
      "16     ðŸŒŸ         38\n",
      "17     ðŸ¤©         36\n",
      "18     ðŸ˜„         36\n",
      "19     ðŸ¤—         35\n"
     ]
    }
   ],
   "source": [
    "# 3. check most frequent emojis\n",
    "import emoji\n",
    "\n",
    "all_emojis = df['review_text'].apply(lambda t: [c for c in t if c in emoji.EMOJI_DATA]).explode().dropna()\n",
    "emoji_counts = all_emojis.value_counts().head(20).rename_axis('Emoji').reset_index(name='Frequency')\n",
    "\n",
    "print('\\Most frequent emojis:')\n",
    "print(emoji_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e25182",
   "metadata": {},
   "source": [
    "# Normalization maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8dbf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_map = {\n",
    "'ðŸ‘': ' bagus ', 'ðŸ’¯': ' bagus ', 'ðŸ˜¡': ' marah ', 'ðŸ˜ ': ' marah ',\n",
    "'â­': ' bintang ', 'ðŸŒŸ': ' bintang ', 'â¤ï¸': ' suka ', 'â¤': ' suka ',\n",
    "'ðŸ¥°': ' suka ', 'ðŸ˜': ' suka ', 'ðŸ¤©': ' suka ', 'ðŸ˜˜': ' suka ',\n",
    "'ðŸ«°': ' suka ', 'ðŸ‘Œ': ' oke ', 'ðŸ˜Š': ' senang ', 'ðŸ˜': ' senang ',\n",
    "'ðŸ¤£': ' lucu ', 'ðŸ™': ' terima kasih ', 'ðŸ˜”': ' kecewa ', 'ðŸ‘Ž': ' buruk '\n",
    "}\n",
    "\n",
    "\n",
    "key_norm = {\n",
    "'yg': 'yang', 'gan': 'juragan', 'n': 'dan', 'bgt': 'banget', 'nga': 'tidak',\n",
    "'gk': 'tidak', 'gak': 'tidak', 'ga': 'tidak', 'nggak': 'tidak', 'shoppee': 'shopee',\n",
    "'sdh': 'sudah', 'udah': 'sudah', 'blm': 'belum', 'dgn': 'dengan',\n",
    "'tlg': 'tolong', 'brg': 'barang', 'sy': 'saya', 'kalo': 'kalau',\n",
    "'klo': 'kalau', 'jm': 'jam', 'bs': 'bisa', 'krn': 'karena', 'lelet': 'lambat',\n",
    "'tp': 'tapi', 'dr': 'dari', 'kpn': 'kapan', 'jg': 'juga', 'gede': 'besar',\n",
    "'ok': 'oke', 'thx': 'terima kasih', 'plis': 'tolong', 'bener': 'benar',\n",
    "'good': 'bagus', 'bagu': 'bagus', 'mantap': 'bagus', 'cepet': 'cepat',\n",
    "'jelek': 'buruk', 'lemot': 'lambat', 'seneng': 'senang'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55718ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_regex(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # 1. lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. replace Emojis using emoji_map\n",
    "    for emo, replacement in emoji_map.items():\n",
    "        text = text.replace(emo, replacement)\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    \n",
    "    # 4. delete URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 5. delete mention (@) and hashtag (#)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # 6. delete any special characters and numbers (only keep letters a-z and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 7. delete excessive repeated characters (ex: \"baguuusss\" -> \"bagus\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    # 8. delete excessive spaces (multiple spaces -> single space)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_slang_fast(text):\n",
    "    \"\"\"Mengganti slang menggunakan Dictionary Lookup (Sangat Cepat).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # split sentence to words\n",
    "    words = text.split()\n",
    "    \n",
    "    # list comprehension: check dictionary, if exists replace, else keep\n",
    "    normalized_words = [key_norm.get(word, word) for word in words]\n",
    "    \n",
    "    return ' '.join(normalized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b8e3e",
   "metadata": {},
   "source": [
    "# Setup Stopwords & Stemmer (Sastrawi)\n",
    "For sentiment analysis, it's best not to remove negative words like \"tidak\" \"bukan,\" or \"jangan.\" Sastrawi removes \"tidak\" by default, so it needs to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5864294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup stopwords\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()\n",
    "\n",
    "# exclude negative words from stopwords\n",
    "excluded_stopwords = ['tidak', 'bukan', 'jangan', 'belum', 'tapi']\n",
    "stopwords = [word for word in stopwords if word not in excluded_stopwords]\n",
    "\n",
    "# setup stemmer\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    return ' '.join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "def stemming_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    return stemmer.stem(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5ec46",
   "metadata": {},
   "source": [
    "# Pipeline Execution (Batch Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1537526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning process... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef09131a1f7a4763affb3ecae9f0ecd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61657f0b7e3b4e95838df49c791a2ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123d7b1cfb7345ada416fc7851d13726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96eef190df9b41c8805e12b11e45cc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning process completed!\n",
      "\n",
      "Data Comparison:\n",
      "                                         review_text  \\\n",
      "0  padahal aku nyamaaan banget belaja disini tapi...   \n",
      "1                                       bagus bangat   \n",
      "2  semua oke sih, cuma saat saya mau ngasih ulasa...   \n",
      "3                                 mudah simpel cepat   \n",
      "4  maaf untuk sementara saya kasih bintang 1, kar...   \n",
      "5                                              bagus   \n",
      "6  aplikasi aneh, transaksi selalu dibatalkan sys...   \n",
      "7                                             mantap   \n",
      "8  sistem sangat bobrok, berulangkali mendapat or...   \n",
      "9  Akun tiba tiba diblokir tidak ada penjelasan, ...   \n",
      "\n",
      "                                          text_clean  \n",
      "0  padahal aku nyaman banget baja sini tapi opsi ...  \n",
      "1                                       bagus bangat  \n",
      "2  semua oke sih cuma mau ngasih ulas unggah vide...  \n",
      "3                                 mudah simpel cepat  \n",
      "4  maaf kasih bintang beku akun lama banget buk p...  \n",
      "5                                              bagus  \n",
      "6  aplikasi aneh transaksi selalu batal system pa...  \n",
      "7                                              bagus  \n",
      "8  sistem sangat bobrok berulangkali dapat order ...  \n",
      "9  akun tiba tiba blokir tidak jelas beli hari no...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Starting cleaning process... \")\n",
    "\n",
    "# apply clean text (normalize & regex)\n",
    "df['text_clean'] = df['review_text'].progress_apply(clean_text_regex)\n",
    "df['text_clean'] = df['text_clean'].progress_apply(normalize_slang_fast)\n",
    "\n",
    "# apply stopword removal\n",
    "df['text_clean'] = df['text_clean'].progress_apply(remove_stopwords)\n",
    "\n",
    "# apply stemming\n",
    "df['text_clean'] = df['text_clean'].progress_apply(stemming_text) \n",
    "\n",
    "print(\"Cleaning process completed!\")\n",
    "\n",
    "# compare before vs after\n",
    "print(\"\\nData Comparison:\")\n",
    "print(df[['review_text', 'text_clean']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c53b1",
   "metadata": {},
   "source": [
    "# Measuring Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73df661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First vocabulary: 187647\n",
      "Final vocabulary: 11020\n",
      "Noise Reduction Efficiency: 94.13%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>makin gacor..wkwk</td>\n",
       "      <td>makin gacorwkwk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18050</th>\n",
       "      <td>sumpah Shopee bagus tpÂ² kenapa tibaÂ² saya disu...</td>\n",
       "      <td>sumpah shopee bagus tapi tiba suruh bayar mulu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7050</th>\n",
       "      <td>cukup puas untuk kedepan nya semoga shopee leb...</td>\n",
       "      <td>cukup puas depan nya moga shopee lebih baik mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9379</th>\n",
       "      <td>banyak penipuan</td>\n",
       "      <td>banyak tipu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11883</th>\n",
       "      <td>sangat membantu.blanja jdi mudah dari berbagai...</td>\n",
       "      <td>sangat membantublanja jdi mudah bagai kalang</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             review_text  \\\n",
       "2789                                   makin gacor..wkwk   \n",
       "18050  sumpah Shopee bagus tpÂ² kenapa tibaÂ² saya disu...   \n",
       "7050   cukup puas untuk kedepan nya semoga shopee leb...   \n",
       "9379                                     banyak penipuan   \n",
       "11883  sangat membantu.blanja jdi mudah dari berbagai...   \n",
       "\n",
       "                                              text_clean  \n",
       "2789                                     makin gacorwkwk  \n",
       "18050  sumpah shopee bagus tapi tiba suruh bayar mulu...  \n",
       "7050   cukup puas depan nya moga shopee lebih baik mu...  \n",
       "9379                                         banyak tipu  \n",
       "11883       sangat membantublanja jdi mudah bagai kalang  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count unique words in cleaned text\n",
    "clean_vocab = set()\n",
    "for text in df['text_clean']:\n",
    "    for word in text.split():\n",
    "        clean_vocab.add(word)\n",
    "\n",
    "print(f\"First vocabulary: {len(raw_all_words)}\")\n",
    "print(f\"Final vocabulary: {len(clean_vocab)}\")\n",
    "print(f\"Noise Reduction Efficiency: {((len(raw_all_words) - len(clean_vocab)) / len(raw_all_words)) * 100:.2f}%\")\n",
    "\n",
    "# display random 5 samples of before vs after cleaning\n",
    "comparison = df[['review_text', 'text_clean']].sample(5)\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa12e1",
   "metadata": {},
   "source": [
    "# Save Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0a10950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data saved at: ../data/processed/shopee_reviews_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# save clean data\n",
    "output_path = '../data/processed/shopee_reviews_clean.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Clean data saved at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopee-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
